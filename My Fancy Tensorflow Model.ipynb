{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2cb18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-1.30.0-py3-none-any.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Flask<3 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (8.0.1)\n",
      "Collecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (0.4.1)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (4.4.4)\n",
      "Requirement already satisfied: alembic<2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (1.7.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (2.26.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (3.19.6)\n",
      "Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (1.4.39)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: numpy<2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (1.21.5)\n",
      "Requirement already satisfied: scipy<2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (1.7.3)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks-cli-0.17.5.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gunicorn<21 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: pytz<2023 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (2021.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<6,>=3.7.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (4.8.1)\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Downloading prometheus_flask_exporter-0.22.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cloudpickle<3 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: packaging<22 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (21.0)\n",
      "Requirement already satisfied: pandas<2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from mlflow) (1.3.5)\n",
      "Requirement already satisfied: importlib-resources in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from alembic<2->mlflow) (5.2.2)\n",
      "Requirement already satisfied: Mako in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from alembic<2->mlflow) (1.2.4)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.1.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from docker<7,>=4.0.0->mlflow) (1.2.1)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from Flask<3->mlflow) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from Flask<3->mlflow) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from Flask<3->mlflow) (2.2.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.5.0)\n",
      "Requirement already satisfied: setuptools>=3.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from gunicorn<21->mlflow) (65.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow) (3.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from packaging<22->mlflow) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from pandas<2->mlflow) (2.8.2)\n",
      "Requirement already satisfied: prometheus-client in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from prometheus-flask-exporter<1->mlflow) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.17.3->mlflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.17.3->mlflow) (3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from sqlalchemy<2,>=1.4.0->mlflow) (1.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages (from Jinja2>=3.0->Flask<3->mlflow) (2.1.2)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.5-py3-none-any.whl size=143001 sha256=eb1b8525fb702960587d9369d328f7bdf5727d7fc50d590c14c20b51fe150174\n",
      "  Stored in directory: /Users/stevebarnard/Library/Caches/pip/wheels/6c/ff/c4/ebb0ffa080503b245ed720b322126e7e24098fca252f0db8f2\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: smmap, querystring-parser, gitdb, gitpython, databricks-cli, prometheus-flask-exporter, mlflow\n",
      "Successfully installed databricks-cli-0.17.5 gitdb-4.0.10 gitpython-3.1.31 mlflow-1.30.0 prometheus-flask-exporter-0.22.3 querystring-parser-1.2.4 smmap-5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb6ea7",
   "metadata": {},
   "source": [
    "To package up a TensorFlow model into an MLflow package with accompanying code examples, you can follow these general steps:\n",
    "\n",
    "Train and save your TensorFlow model: Train your TensorFlow model and save it in a format that can be loaded later. For example, you can use ```tf.saved_model.save``` or ```model.save```.\n",
    "\n",
    "Set up MLflow tracking: Initialize an MLflow tracking server or set up a local tracking directory to store your experiment runs and artifacts.\n",
    "\n",
    "Create an MLflow project: Create an MLflow project with a directory structure that includes your TensorFlow model and any necessary dependencies or code examples. The directory structure should look something like this:\n",
    "\n",
    "```\n",
    "my_project/\n",
    "├── MLproject\n",
    "├── conda.yaml\n",
    "├── code/\n",
    "│   ├── train.py\n",
    "│   ├── predict.py\n",
    "│   └── ...\n",
    "└── model/\n",
    "    └── my_model/\n",
    "        ├── saved_model.pb\n",
    "        └── variables/\n",
    "            ├── variables.data-00000-of-00001\n",
    "            └── variables.index\n",
    "```\n",
    "Define the MLproject file: In the MLproject file, define the entry points for training and predicting with your TensorFlow model. You can also specify the required parameters and dependencies.\n",
    "\n",
    "Create a conda environment file: In the conda.yaml file, define the required dependencies for your project.\n",
    "\n",
    "Write code examples: In the code directory, write code examples that demonstrate how to use your TensorFlow model for training and prediction. These examples should use the entry points defined in the MLproject file.\n",
    "\n",
    "Package up the project: Package up your project as an MLflow artifact by running mlflow projects pack my_project. This will create a .tar.gz file that includes your TensorFlow model, code examples, and dependencies.\n",
    "\n",
    "Publish the artifact: Publish your MLflow artifact to a registry so that others can access and use your TensorFlow model.\n",
    "\n",
    "Here's an example of what the MLproject file might look like:\n",
    "```\n",
    "name: my_project\n",
    "conda_env: conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  train:\n",
    "    command: \"python code/train.py --data-path {data_path} --model-path {model_path}\"\n",
    "    parameters:\n",
    "      data_path: {type: str, default: data/}\n",
    "      model_path: {type: str, default: model/}\n",
    "  predict:\n",
    "    command: \"python code/predict.py --model-path {model_path} --input-path {input_path} --output-path {output_path}\"\n",
    "    parameters:\n",
    "      model_path: {type: str, default: model/}\n",
    "      input_path: {type: str, default: data/test.tfrecord}\n",
    "      output_path: {type: str, default: predictions.csv}\n",
    "```\n",
    "In this example, there are two entry points: train and predict. The train entry point runs the train.py script and takes two parameters: data_path and model_path. The predict entry point runs the predict.py script and takes three parameters: model_path, input_path, and output_path. These parameters are specified as command line arguments and can be set when running the MLflow project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d309492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d634e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b64f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e098f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a821ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d49e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(texts, tokenizer, train=True, max_seq_length=None):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    if train:\n",
    "        max_seq_length = np.max(list(map(lambda x: len(x) , sequences)))\n",
    "    \n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ce849e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['v1'] = df['v1'].apply(lambda x: 1 if x =='spam' else 0)\n",
    "     \n",
    "    # split into X and y\n",
    "    y = df.iloc[:,0] \n",
    "    X = df.iloc[:,1]\n",
    "    \n",
    "    # create train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.7, shuffle=True, random_state=1234) #shuffle again\n",
    "    \n",
    "    # create and fit tokenizers\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=8000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    X_train = get_sequences(X_train, tokenizer, train=True)\n",
    "    X_test = get_sequences(X_test, tokenizer, train=False, max_seq_length=X_train.shape[1])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5634b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, tokenizer = preprocess_inputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ca9bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "172e6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim = tokenizer.num_words, # to match num of unique words in subject line\n",
    "    output_dim = 64\n",
    ")(inputs)\n",
    "\n",
    "# so we have an input of 8000 neurons, going into 64 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c03fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = tf.keras.layers.Flatten()(embedding) # unstack amtrix into a vector so a ton of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb41e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(flatten) # check out relu use N nodes for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88d59a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a228249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy', # 'sparse_categorical_crossentropy' for multiclass\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf169d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 189)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 189, 64)           512000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12096)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 12097     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 524,097\n",
      "Trainable params: 524,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1efc6c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - 1s 9ms/step - loss: 0.3082 - accuracy: 0.8750 - auc: 0.8269 - val_loss: 0.1698 - val_accuracy: 0.9474 - val_auc: 0.9545\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0852 - accuracy: 0.9792 - auc: 0.9895 - val_loss: 0.0876 - val_accuracy: 0.9744 - val_auc: 0.9819\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0379 - accuracy: 0.9885 - auc: 0.9983 - val_loss: 0.0704 - val_accuracy: 0.9795 - val_auc: 0.9872\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0205 - accuracy: 0.9952 - auc: 0.9995 - val_loss: 0.0634 - val_accuracy: 0.9846 - val_auc: 0.9893\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0127 - accuracy: 0.9978 - auc: 0.9999 - val_loss: 0.0622 - val_accuracy: 0.9833 - val_auc: 0.9908\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0081 - accuracy: 0.9987 - auc: 0.9999 - val_loss: 0.0672 - val_accuracy: 0.9833 - val_auc: 0.9829\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9994 - auc: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9821 - val_auc: 0.9746\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9994 - auc: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9821 - val_auc: 0.9746\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split= 0.2,\n",
    "    batch_size = 32,\n",
    "    epochs = 100,\n",
    "    callbacks= [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "            patience= 3,\n",
    "            restore_best_weights= True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a039a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b1f49c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Loss: 0.0628\n",
      "Test Accuracy: 98.33%\n",
      "     Test AUC: 0.9898\n"
     ]
    }
   ],
   "source": [
    "print(\"    Test Loss: {:.4f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n",
    "print(\"     Test AUC: {:.4f}\".format(results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ae3a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: FancySpamModel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: FancySpamModel/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(filepath='FancySpamModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "738582fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('FancySpamModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0872ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 986us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.squeeze(np.array(new_model.predict(X_test) >= 0.5, dtype= np.int)) != y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "514947db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevebarnard/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.squeeze(np.array(model.predict(X_test) >= 0.5, dtype= np.int)) != y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8751c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking good, now if I can get this up to garden, then pull down and get matching results we have a success!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
